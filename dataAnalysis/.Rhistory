trial_data_wide_training <- create_wide_data(nback_trials_training, "generalNbackCounter")
create_wide_data <- function(data, counter_col) {
if(nrow(data) == 0) return(data.frame())
data %>%
select(all_of(counter_col), block, task) %>%
# Create unique identifier for block columns
mutate(trial_id = paste0("trial_", task, "_", .data[[counter_col]], "_block")) %>%
select(trial_id, block) %>%
# group_by(trial_id) %>%
pivot_wider(names_from = trial_id, values_from = block)
}
block_data_wide_main <- create_wide_data(nback_trials, "mainNbackCounter")
block_data_wide_training <- create_wide_data(nback_trials_training, "generalNbackCounter")
create_wide_data <- function(data, counter_col) {
if(nrow(data) == 0) return(data.frame())
data %>%
select(all_of(counter_col), key_press, task) %>%
# Create unique identifier for key_press columns
mutate(trial_id = paste0("trial_", task, "_", .data[[counter_col]], "_key_press")) %>%
select(trial_id, key_press) %>%
pivot_wider(names_from = trial_id, values_from = key_press)
}
keypress_data_wide_main <- create_wide_data(nback_trials, "mainNbackCounter")
keypress_data_wide_training <- create_wide_data(nback_trials_training, "generalNbackCounter")
duplicate_cols <- names(trial_data_wide_main)[duplicated(names(trial_data_wide_main))]
if(length(duplicate_cols) > 0) {
cat("Found duplicate column names:\n")
print(duplicate_cols)
} else {
cat("No duplicate column names found\n")
}
# Create the final row for this participant
participant_row <- data.frame(
subject = subject_id,
STAT_accuracy_nback = stat_accuracy_nback,
STAT_accuracy_nbackVisual = stat_accuracy_nbackVisual,
STAT_accuracy_nback_hard = stat_accuracy_nback_hard,
STAT_accuracy_nback_easy = stat_accuracy_nback_easy,
STAT_accuracy_nbackVisual_hard = stat_accuracy_nbackVisual_hard,
STAT_accuracy_nbackVisual_easy = stat_accuracy_nbackVisual_easy,
STAT_accuracy_2firstVisualTrials = stat_accuracy_2firstVisualTrials,
STAT_accuracy_without_2firstVisualTrials = stat_accuracy_without_2firstVisualTrials,
STAT_accuracy_without_2firstVisualTrials_1stBlock = stat_accuracy_without_2firstVisualTrials_1stBlock,
STAT_accuracy_without_2firstVisualTrials_2ndBlock = stat_accuracy_without_2firstVisualTrials_2ndBlock,
STAT_accuracy_2firstVisualTrials_1stBlock = stat_accuracy_2firstVisualTrials_1stBlock,
STAT_accuracy_2firstVisualTrials_2ndBlock = stat_accuracy_2firstVisualTrials_2ndBlock,
STAT_accuracy_afterVisualTrials = stat_accuracy_afterVisualTrials,
STAT_accuracy_afterVisualTrials_hard = stat_accuracy_afterVisualTrials_hard,
STAT_accuracy_afterVisualTrials_easy = stat_accuracy_afterVisualTrials_easy,
STAT_accuracy_nbackVisualPractice = stat_accuracy_nbackVisualPractice,
STAT_accuracy_nbackPractice_hard = stat_accuracy_nbackPractice_hard,
STAT_accuracy_nbackPractice_easy = stat_accuracy_nbackPractice_easy,
STAT_accuracy_nbackOverallTraining_hard = stat_accuracy_nbackOverallTraining_hard,
STAT_accuracy_nbackOverallTraining_easy = stat_accuracy_nbackOverallTraining_easy,
repeated_comprehension_question_easy = as.numeric(all_correct_false_count["comprehensionSurveyEasy"]),
repeated_comprehension_question_hard = as.numeric(all_correct_false_count["comprehensionSurveyHard"]),
trained_easy_nth = as.numeric(number_training_block["practice_easy"]),
trained_hard_nth = as.numeric(number_training_block["practice_hard"]),
trained_visual_nth = as.numeric(number_training_block["nbackVisual_practice"]),
treatment_order = block_order,
payment = totPay,
selected_block = selSub
)
# Combine with trial data
if(ncol(trial_data_wide_main) > 0) {
participant_row_main <- cbind(participant_row, trial_data_wide_main)
}
if(ncol(trial_data_wide_training) > 0) {
participant_row_training <- cbind(participant_row, trial_data_wide_training)
}
if(ncol(block_data_wide_main) > 0) {
participant_row_main <- cbind(participant_row_main, block_data_wide_main)
}
if(ncol(block_data_wide_training) > 0) {
participant_row_training <- cbind(participant_row_training, block_data_wide_training)
}
if(ncol(keypress_data_wide_main) > 0) {
participant_row_main <- cbind(participant_row_main, keypress_data_wide_main)
}
if(ncol(keypress_data_wide_training) > 0) {
participant_row_training <- cbind(participant_row_training, keypress_data_wide_training)
}
if(ncol(demographics_df) > 0) {
participant_row_main <- cbind(participant_row_main, demographics_df)
participant_row_training <- cbind(participant_row_training, demographics_df)
}
# Create dynamically named dataset for this participant
dataset_name_main <- paste0("final_data_main_", iSub)
assign(dataset_name_main, participant_row_main)
dataset_name_training <- paste0("final_data_training_", subject_id)
assign(dataset_name_training, participant_row_training)
# Add to final_data
if(afterVisualAnswers <= 12){
if (iSub == 1) {
final_data_main <- participant_row_main
} else {
final_data_main <- rbind(final_data_main, participant_row_main)
}
}
# # Optional: Also save as CSV file
# csv_filename <- paste0("final_data_", iSub, ".csv")
# write.csv(participant_row, csv_filename, row.names = FALSE)
}
view(final_data_main)
view(final_data_main)
# Create df_model for linear models from final_data - reshape from wide to long
df_model <- final_data_main %>%
# Select relevant columns and demographic data
select(subject, treatment_order, demo_age, demo_occu, demo_gend, demo_educ,
demo_lsat, demo_reve, repeated_comprehension_question_easy, repeated_comprehension_question_hard,
trained_easy_nth, trained_hard_nth, trained_visual_nth,
starts_with("trial_")) %>%
# Reshape to long format - each trial becomes a row
pivot_longer(
cols = starts_with("trial_") & !ends_with("_block") &!ends_with("key_press"), # exclude block columns because not the same class
names_to = "trial_info",
values_to = "value"
) %>%
# Separate the trial information
separate(trial_info,
into = c("trial", "task", "trial_number", "measure"),
sep = "_",
extra = "merge") %>%
# Check for duplicates
# duplicates <- df_model %>%
#   group_by(subject, task, trial_number, measure) %>%
#   summarise(n = n(), .groups = 'drop') %>%
#   filter(n > 1)
# if(nrow(duplicates) > 0) {
#   cat("Found duplicates:\n")
#   print(n = 100, duplicates)
# }
# duplicates_check <- df_model %>%
#   filter(subject == "nc8np2p74lsmype", task == "nback", measure == "after_visual", trial_number == 1)
# duplicates_check1 <- which(
#   df_model$subject == "nc8np2p74lsmype" &
#     df_model$task == "nback" &
#     df_model$measure == "after_visual" &
#     df_model$trial_number == 1
# )
# rowPerSubj <- df_model %>%
#   filter(subject == "tlxaz39vhdk444o")%>%
#   nrow()
# rowPerSubj # 2460
# nrow(df_model)
# Pivot wider to get measures as columns
pivot_wider(
names_from = measure,
values_from = value
) %>%
# Extract block information from block columns in final_data
left_join( final_data_main %>%
select(subject, starts_with("trial_") & ends_with("_block") & !ends_with("first_block")) %>%
pivot_longer(
cols = starts_with("trial_") & ends_with("_block"),
names_to = "trial_info",
values_to = "block"
) %>%
separate(trial_info,
into = c("trial", "task", "trial_number", "block_label"),
sep = "_") %>%
select(subject, task, trial_number, block),
by = c("subject", "task", "trial_number")
) %>%
left_join( final_data_main %>%
select(subject, starts_with("trial_") & ends_with("key_press")) %>%
pivot_longer(
cols = starts_with("trial_") & ends_with("key_press"),
names_to = "trial_info",
values_to = "key_press"
) %>%
separate(trial_info,
into = c("trial", "task", "trial_number", "key_label"),
sep = "_",
extra = "merge") %>%
select(subject, task, trial_number, key_press),
by = c("subject", "task", "trial_number")
) %>%
# Add is_first_block logic
mutate(
is_first_block = case_when(
block == "main_easy" & treatment_order == "easy_first" ~ 1,
block == "main_hard" & treatment_order == "hard_first" ~ 1,
block == "main_easy" & treatment_order == "hard_first" ~ 0,
block == "main_hard" & treatment_order == "easy_first" ~ 0,
TRUE ~ NA_real_
),
# add an is_correct column to fit linear models
is_correct_excluding = case_when( # excluding : no responses = NA . Incorrect  = miss or f-a
miss == 1 | false_alarm == 1 ~ 0,
hit == 1 | correct_rejection == 1 ~ 1,
hit == 0 & false_alarm == 0 & hit == 0 & correct_rejection == 0 ~ NA_real_
),
is_correct_including = case_when( # including : no responses = incorrect
miss == 1 | false_alarm == 1 ~ 0,
hit == 1 | correct_rejection == 1 ~ 1,
hit == 0 & false_alarm == 0 & hit == 0 & correct_rejection == 0 ~ 0,
),
# Convert to appropriate types
demo_age = as.numeric(demo_age),
demo_occu = as.factor(demo_occu),
demo_gend = as.factor(demo_gend),
demo_educ = as.factor(demo_educ),
block = as.factor(block),
task = as.factor(task),
trial_number = as.numeric(trial_number)
) %>%
# Select final columns in desired order
select(subject, block, is_first_block, demo_age, demo_occu, demo_gend, demo_educ,
task, trial_number, miss, hit, false_alarm, correct_rejection,
is_correct_excluding, is_correct_including, key_press,
repeated_comprehension_question_easy,
repeated_comprehension_question_hard, trained_easy_nth,
trained_hard_nth, trained_visual_nth) #%>%
df_model
# Define the chance levels for different strategies
chance_level <- function (propTa, strat, task) {
if (strat == "stratF"){
chance_level <- 1- propTa
}
else if (strat == "stratJ"){
chance_level <- propTa
}
else if (strat == "stratR"){
chance_level <- 0.5
}
else if (strat == "stratF-2" & task == "nbackVisual"){
chance_level <- 0.2 + 0.8*(1- propTa)
}
else if (strat == "stratJ-2" & task == "nbackVisual"){
chance_level <- 0.2 + 0.8*(propTa)
}
else if (strat == "stratR-2" & task == "nbackVisual"){
chance_level <- 0.6
}
}
compare_chance_levels <- function() {
# Define parameters
propTa_nback <- 20/63
propTa_nbackVisual <- 3/10
# Calculate all chance levels
results <- data.frame(
Strategy = c("stratF", "stratJ", "stratR", "stratF", "stratJ", "stratR", "stratF-2", "stratJ-2", "stratR-2"),
Task = c("nback", "nback", "nback", "nbackVisual", "nbackVisual", "nbackVisual", "nbackVisual", "nbackVisual", "nbackVisual"),
Chance_Level = c(
chance_level(propTa_nback, "stratF", "nback"),
chance_level(propTa_nback, "stratJ", "nback"),
chance_level(propTa_nback, "stratR", "nback"),
chance_level(propTa_nbackVisual, "stratF", "nbackVisual"),
chance_level(propTa_nbackVisual, "stratJ", "nbackVisual"),
chance_level(propTa_nbackVisual, "stratR", "nbackVisual"),
chance_level(propTa_nbackVisual, "stratF-2", "nbackVisual"),
chance_level(propTa_nbackVisual, "stratJ-2", "nbackVisual"),
chance_level(propTa_nbackVisual, "stratR-2", "nbackVisual")),
stringsAsFactors = FALSE
)
results <- results %>%
arrange(Task, Chance_Level)
cat("For nback task:\n")
nback_results <- results %>% filter(Task == "nback")
print(nback_results %>% select(Strategy, Chance_Level))
cat("\nFor nbackVisual task:\n")
visual_results <- results %>% filter(Task == "nbackVisual")
print(visual_results %>% select(Strategy, Chance_Level))
cat("• Best strategy for nback:", results$Strategy[which.max(nback_results$Chance_Level)],
"with", round(max(nback_results$Chance_Level), 3), "accuracy\n")
cat("• Worst strategy for nback:", nback_results$Strategy[which.min(nback_results$Chance_Level)],
"with", round(min(nback_results$Chance_Level), 3), "accuracy\n")
cat("• Best strategy for nbackVisual:", visual_results$Strategy[which.max(visual_results$Chance_Level)],
"with", round(max(visual_results$Chance_Level), 3), "accuracy\n")
cat("• Worst strategy for nbackVisual:", visual_results$Strategy[which.min(visual_results$Chance_Level)],
"with", round(min(visual_results$Chance_Level), 3), "accuracy\n")
return(results)
}
chance_comparison <- compare_chance_levels()
# Extract the values you need from the returned data
nback_results <- chance_comparison %>% filter(Task == "nback")
visual_results <- chance_comparison %>% filter(Task == "nbackVisual")
accuracy_best_strat_nback <- max(nback_results$Chance_Level)
accuracy_worst_strat_nback <- min(nback_results$Chance_Level)
accuracy_best_strat_nbackVisual <- max(visual_results$Chance_Level)
accuracy_worst_strat_nbackVisual <- min(visual_results$Chance_Level)
# Assess the use of non-random strategies (press only f or only j) for all conditions:
key_press_summary <- df_model %>%
filter(!is.na(key_press)) %>%
group_by(task, block) %>%
summarise(
total_trials = n(),
f_presses = sum(key_press == "f", na.rm = TRUE),
j_presses = sum(key_press == "j", na.rm = TRUE),
proportion_f = f_presses / total_trials,
proportion_j = j_presses / total_trials,
.groups = 'drop'
)
print("Key press summary by task and block:")
print(key_press_summary)
# Scatter plot: Easy vs Hard nbackVisual accuracy for each participant
scatter_plot <- ggplot(final_data_main, aes(x = STAT_accuracy_nbackVisual_easy, y = STAT_accuracy_nbackVisual_hard)) +
# Add diagonal line (equal performance line)
geom_abline(
intercept = 0,
slope = 1,
linetype = "dashed",
color = "gray50",
linewidth = 1,
alpha = 1
) +
# Add individual participant points
geom_point(
size = 3,
alpha = 1,
color = "#2E86AB"  # Nice blue color
) +
# Add chance level lines
geom_hline(
yintercept = accuracy_best_strat_nbackVisual,
linetype = "dotted",
color = "red",
linewidth = 0.8
) +
geom_vline(
xintercept = accuracy_best_strat_nbackVisual,
linetype = "dotted",
color = "red",
linewidth = 0.8
) +
# Add chance level annotation
annotate(
"text",
x = 0.05,
y = accuracy_best_strat_nbackVisual + 0.02,
label = paste0("Chance level = ", round(accuracy_best_strat_nbackVisual, 3)),
size = 3,
color = "red",
hjust = 0
) +
# Customize axes and theme
scale_x_continuous(
limits = c(0, 0.99),
breaks = seq(0, 1, 0.2),
labels = scales::percent_format(accuracy = 1),
expand = c(0, 0)
) +
scale_y_continuous(
limits = c(0, 0.99),
breaks = seq(0, 1, 0.2),
labels = scales::percent_format(accuracy = 1),
expand = c(0, 0)
) +
coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +
labs(
title = "Individual Performance: Easy vs Hard nbackVisual",
subtitle = paste0("n = ", nrow(final_data), " participants"),
x = "Accuracy in Easy Block",
y = "Accuracy in Hard Block",
caption = "Dashed line = equal performance | Dotted lines = theoretical chance level"
) +
theme_pubr() +
theme(
plot.title = element_text(hjust = 0.5),
plot.subtitle = element_text(hjust = 0.5),
aspect.ratio = 1  # Make the plot square
)
print(scatter_plot)
ggsave("scatter_easy_vs_hard_nbackVisual.png", scatter_plot, width = 8, height = 8, dpi = 300)
create_comparison_barplot <- function(data, easy_col, hard_col, task_name, chance_level) {
# Prepare data
plot_data <- data %>%
select(subject, all_of(easy_col), all_of(hard_col), treatment_order) %>%
pivot_longer(
cols = c(all_of(easy_col), all_of(hard_col)),
names_to = "condition",
values_to = "accuracy"
) %>%
mutate(
condition = case_when(
str_detect(condition, "hard") ~ "Hard",
str_detect(condition, "easy") ~ "Easy"
),
condition = factor(condition, levels = c("Easy", "Hard")),
point_color = case_when(
condition == "Hard" & treatment_order == "hard_first" ~ "white",
condition == "Hard" & treatment_order == "easy_first" ~ "black",
condition == "Easy" & treatment_order == "hard_first" ~ "black",
condition == "Easy" & treatment_order == "easy_first" ~ "white",
TRUE ~ "red"  # fallback for any unexpected cases
))
# Calculate stats
desc_stats <- plot_data %>%
group_by(condition) %>%
summarise(
mean = mean(accuracy, na.rm = TRUE),
sd = sd(accuracy, na.rm = TRUE),
n = n(),
se = sd / sqrt(n),
.groups = 'drop'
)
stat_test <- plot_data %>%
t_test(accuracy ~ condition, paired = TRUE) %>%
add_significance()
p <- ggplot(plot_data, aes(x = condition, y = accuracy)) +
# Add bars manually to avoid conflict
stat_summary(
fun = mean,
geom = "bar",
aes(fill = condition),
alpha = 0.7,
width = 0.6,
color = "black",
linewidth = 0.8
) +
# Add error bars
stat_summary(
fun.data = mean_se,
geom = "errorbar",
color = "black",
width = 0.2,
linewidth = 1
) +
# Add points with treatment order colors
geom_jitter(
aes(fill = point_color),
width = 0.2,
height = 0,
size = 3,
alpha = 0.9,
stroke = 1.5,
shape = 21,  # Circle with separate fill and border
color = "black"  # Black border
) +
# Set bar colors
scale_fill_manual(
values = c("Easy" = "#00AFBB", "Hard" = "#E7B800",
"white" = "white", "black" = "black", "gray" = "gray"),
guide = "none"
) +
geom_hline(
yintercept = chance_level,
linetype = "dashed",
color = "red",
linewidth = 1
) +
annotate(
"text",
x = 1.5,
y = chance_level + 0.02,
label = paste0("Chance level = ", round(chance_level, 3)),
color = "red",
size = 3.5
) +
stat_pvalue_manual(
stat_test,
label = "p = {p}",
tip.length = 0.01,
y.position = 0.95
) +
labs(
title = paste("Accuracy in", task_name, "Task: Hard vs Easy Blocks"),
subtitle = paste0("n = ", desc_stats$n[1], " participants"),
x = "Block Difficulty",
y = "Accuracy",
caption = paste0("Error bars represent standard error of the mean | Red line = theoretical chance level\n",
"Point colors: White = block done first, Black = block done second")
) +
theme_pubr() +
theme(
legend.position = "none",
plot.title = element_text(hjust = 0.5),
plot.subtitle = element_text(hjust = 0.5)
) +
scale_y_continuous(
limits = c(0, 1),
breaks = seq(0, 1, 0.1),
labels = scales::percent_format(accuracy = 1)
)
# Print point color summary
color_summary <- plot_data %>%
group_by(condition, treatment_order, point_color) %>%
summarise(n = n(), .groups = 'drop')
cat("\nPoint color mapping:\n")
print(color_summary)
return(list(plot = p, stats = desc_stats, test = stat_test, color_summary = color_summary)) }
# # Define plot specifications
plot_specs <- list(
nbackVisual = list(
easy_col = "STAT_accuracy_nbackVisual_easy",
hard_col = "STAT_accuracy_nbackVisual_hard",
task_name = "nbackVisual",
chance_level = accuracy_best_strat_nbackVisual
),
nback = list(
easy_col = "STAT_accuracy_nback_easy",
hard_col = "STAT_accuracy_nback_hard",
task_name = "nback",
chance_level = accuracy_best_strat_nback
),
afterNbackVisual = list(
easy_col = "STAT_accuracy_afterVisualTrials_easy",
hard_col = "STAT_accuracy_afterVisualTrials_hard",
task_name = "nbackAfterVisualTrials",
chance_level = accuracy_best_strat_nback
)
)
# Generate all plots at oncePui
all_plots <- map(plot_specs, ~create_comparison_barplot(
final_data_main, .$easy_col, .$hard_col, .$task_name, .$chance_level
))
print(all_plots$nbackVisual$plot)
print(all_plots$nback$plot)
print(all_plots$nbackVisual$plot)
print(all_plots$afterNbackVisual$plot)
print(all_plots$nbackVisual$plot)
